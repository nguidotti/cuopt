# SPDX-FileCopyrightText: Copyright (c) 2023-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: Build and push image variant

on:
  workflow_call:
    inputs:
      ARCHES:
        required: true
        type: string
      CUDA_VER:
        required: true
        type: string
      PYTHON_VER:
        required: true
        type: string
      CUOPT_VER:
        required: true
        type: string

jobs:
  build:
    strategy:
      matrix:
        ARCH: ${{ fromJSON(inputs.ARCHES) }}
        CUDA_VER: ["${{ inputs.CUDA_VER }}"]
        PYTHON_VER: ["${{ inputs.PYTHON_VER }}"]
        CUOPT_VER: ["${{ inputs.CUOPT_VER }}"]
      fail-fast: false
    runs-on: "linux-${{ matrix.ARCH }}-cpu4"
    steps:
      - name: Checkout code repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Login to DockerHub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.CUOPT_DOCKERHUB_USERNAME }}
          password: ${{ secrets.CUOPT_DOCKERHUB_TOKEN }}
      - name: Login to NGC
        uses: docker/login-action@v3
        with:
          registry: "nvcr.io"
          username: "$oauthtoken"
          password: ${{ secrets.CUOPT_NGC_DOCKER_KEY }}
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          # Using the built-in config from NVIDIA's self-hosted runners means that 'docker build'
          # will use NVIDIA's self-hosted DockerHub pull-through cache, which should mean faster builds,
          # fewer network failures, and fewer rate-limiting issues
          buildkitd-config: /etc/buildkit/buildkitd.toml
          driver: docker
          endpoint: builders
      - name: Build image and push to DockerHub and NGC
        uses: docker/build-push-action@v6
        with:
          context: context
          file: ./ci/docker/Dockerfile
          target: cuopt
          push: true
          pull: true
          build-args: |
            CUDA_VER=${{ inputs.CUDA_VER }}
            PYTHON_VER=${{ inputs.PYTHON_VER }}
            CUOPT_VER=${{ inputs.CUOPT_VER }}
          tags: nvidia/cuopt:${{ inputs.CUOPT_VER }}-cuda${{ inputs.CUDA_VER }}-${{ matrix.PYTHON_VER }}-${{ matrix.ARCH }}

      - name: Push image to NGC
        run: |
            docker tag nvidia/cuopt:${{ inputs.CUOPT_VER }}-cuda${{ inputs.CUDA_VER }}-${{ matrix.PYTHON_VER }}-${{ matrix.ARCH }} nvcr.io/nvstaging/nvaie/cuopt:${{ inputs.CUOPT_VER }}-cuda${{ inputs.CUDA_VER }}-${{ matrix.PYTHON_VER }}-${{ matrix.ARCH }}
            docker push nvcr.io/nvstaging/nvaie/cuopt:${{ inputs.CUOPT_VER }}-cuda${{ inputs.CUDA_VER }}-${{ matrix.PYTHON_VER }}-${{ matrix.ARCH }}


        
